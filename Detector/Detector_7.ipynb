{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "534d0d0d",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5a4b82",
   "metadata": {},
   "source": [
    "Now that we have completed all of our models, we can finally implement our own sarcasm detector! We will perform this on our best performing model (outside of the BERT), which was a Logistic Regression with a TFIDF vectorization and a tweaked tokenizer. Let's jump right into it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7778ae",
   "metadata": {},
   "source": [
    "As always, the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a42e3f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "import nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce5dbb0",
   "metadata": {},
   "source": [
    "And the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7d4e9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/lokikeeler/Downloads/train-balanced-sarcasm_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffc96770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>year</th>\n",
       "      <th>SUB_2007scape</th>\n",
       "      <th>...</th>\n",
       "      <th>SUB_television</th>\n",
       "      <th>SUB_tf2</th>\n",
       "      <th>SUB_todayilearned</th>\n",
       "      <th>SUB_trees</th>\n",
       "      <th>SUB_ukpolitics</th>\n",
       "      <th>SUB_unitedkingdom</th>\n",
       "      <th>SUB_videos</th>\n",
       "      <th>SUB_worldnews</th>\n",
       "      <th>SUB_wow</th>\n",
       "      <th>SUB_xboxone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11-01</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I don't pay attention to her, but as long as s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016-09-02 10:35:08</td>\n",
       "      <td>do you find ariana grande sexy ?</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  score  ups  \\\n",
       "0      0                                         NC and NH.      2   -1   \n",
       "1      0  You do know west teams play against west teams...     -4   -1   \n",
       "2      0  They were underdogs earlier today, but since G...      3    3   \n",
       "3      0  This meme isn't funny none of the \"new york ni...     -8   -1   \n",
       "4      0  I don't pay attention to her, but as long as s...      0    0   \n",
       "\n",
       "   downs        date          created_utc  \\\n",
       "0     -1  2016-10-01  2016-10-16 23:55:23   \n",
       "1     -1  2016-11-01  2016-11-01 00:24:10   \n",
       "2      0  2016-09-01  2016-09-22 21:45:37   \n",
       "3     -1  2016-10-01  2016-10-18 21:03:47   \n",
       "4      0  2016-09-01  2016-09-02 10:35:08   \n",
       "\n",
       "                                      parent_comment  year  SUB_2007scape  \\\n",
       "0  Yeah, I get that argument. At this point, I'd ...  2016          False   \n",
       "1  The blazers and Mavericks (The wests 5 and 6 s...  2016          False   \n",
       "2                            They're favored to win.  2016          False   \n",
       "3                         deadass don't kill my buzz  2016          False   \n",
       "4                   do you find ariana grande sexy ?  2016          False   \n",
       "\n",
       "   ...  SUB_television  SUB_tf2  SUB_todayilearned  SUB_trees  SUB_ukpolitics  \\\n",
       "0  ...           False    False              False      False           False   \n",
       "1  ...           False    False              False      False           False   \n",
       "2  ...           False    False              False      False           False   \n",
       "3  ...           False    False              False      False           False   \n",
       "4  ...           False    False              False      False           False   \n",
       "\n",
       "   SUB_unitedkingdom  SUB_videos  SUB_worldnews  SUB_wow  SUB_xboxone  \n",
       "0              False       False          False    False        False  \n",
       "1              False       False          False    False        False  \n",
       "2              False       False          False    False        False  \n",
       "3              False       False          False    False        False  \n",
       "4              False       False          False    False        False  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ee2686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns='parent_comment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b087e041",
   "metadata": {},
   "source": [
    "Here is our tokenizer, the same as the one in our Logistic Regression model with the TFIDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94c1c387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lokikeeler/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "ENGLISH_STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "def my_tokenizer(sentence):\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    for punctuation_mark in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation_mark,'')\n",
    "\n",
    "    listofwords = sentence.split(' ')\n",
    "    listofstemmed_words = []\n",
    "\n",
    "    for word in listofwords:\n",
    "        if (not word in ENGLISH_STOP_WORDS) and (word!=''):\n",
    "            # Stem words\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            listofstemmed_words.append(stemmed_word)\n",
    "\n",
    "    return listofstemmed_words  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ae52b",
   "metadata": {},
   "source": [
    "Again we will work with the comment column, and create our train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f4627f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='label')\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11776d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fdc8766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384932, 107)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35622cfa",
   "metadata": {},
   "source": [
    "Now I will apply the same TFIDF vectorizer with the same parameters as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48f6ba12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lokikeeler/anaconda3/envs/capstone/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(384932, 500)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using our custom tokenizer in TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "                       \n",
    "                        min_df=5,\n",
    "                        max_features=500,\n",
    "                        tokenizer=my_tokenizer, \n",
    "                        ngram_range=(1, 3)\n",
    "                        )\n",
    "tfidf.fit(X_train['comment'])\n",
    "\n",
    "X_train_transformed = tfidf.transform(X_train['comment'])\n",
    "X_test_transformed = tfidf.transform(X_test['comment'])\n",
    "\n",
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35324a0",
   "metadata": {},
   "source": [
    "Lastly, fitting the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d89ea850",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d96a4735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b60835",
   "metadata": {},
   "source": [
    "Perfect. Now we are ready for our sarcasm detector! We will create this function which includes the sentence, the vectorizer (tfidf), and the model (logistic regression). Then our fitted model will make a prediction about whether the setence is sarcastic or not and return a 1 (sarcastic) or 0 (not sarcastic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ff5d0af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarcasm_detector(sentence, vectorizer, model):  \n",
    "    vector1 = vectorizer.transform([sentence])\n",
    "    prediction = model.predict_proba(vector1)\n",
    "    if prediction[0][0] < 0.5:\n",
    "        print(\"Sarcastic\")\n",
    "    else:\n",
    "        print(\"Not Sarcastic\")\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cac2d91",
   "metadata": {},
   "source": [
    "For this first sentence, I meant this to be sarcastic. In reality this sentence could be serious or sarcastic, but I'm glad it got it correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "36273bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Sarcastic\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.59201511, 0.40798489]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarcasm_detector(\"That shirt is blue\", tfidf, logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f986d2",
   "metadata": {},
   "source": [
    "Alright, that one was easy, but an important test. Let's give it another sarcastic sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b5e2ae65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarcastic\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.31247678, 0.68752322]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarcasm_detector(\"Great haircut, Paul!\", tfidf, logreg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d43419a",
   "metadata": {},
   "source": [
    "And again it gets it correct! Let's go again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d3ef0272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Sarcastic\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.72186234, 0.27813766]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarcasm_detector(\"I'm looking forward to Demo Day\", tfidf, logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b74228c",
   "metadata": {},
   "source": [
    "Hmmm, this time it got it wrong, but let's see if we slightly tweak this sentence by adding \"totally\" and an exclamation mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e51e7b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarcastic\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.3179921, 0.6820079]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarcasm_detector(\"I'm totally looking forward to Demo Day!\", tfidf, logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fb99ec",
   "metadata": {},
   "source": [
    "Fascinating! With just a couple tweaks it chaged to reading the sarcasm. This shows that the word totally and the exclamation mark has some feature importance in our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2a2279",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae0175",
   "metadata": {},
   "source": [
    "Using the Logistic Regression with a TFIDF vectorizer from our best model in a previous journal, I was able to create a real sarcasm detector function. I inputted sarcastic and non sarcastic sentences and the model guessed many of them correctly. Furthermore, for the one it got incorrect, we were able to add just a couple features that allowed the function to read it correctly, highlighting the importance of feature selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
