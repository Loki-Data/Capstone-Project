{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a42e3f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7d4e9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/lokikeeler/Downloads/train-balanced-sarcasm_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffc96770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>year</th>\n",
       "      <th>SUB_2007scape</th>\n",
       "      <th>...</th>\n",
       "      <th>SUB_television</th>\n",
       "      <th>SUB_tf2</th>\n",
       "      <th>SUB_todayilearned</th>\n",
       "      <th>SUB_trees</th>\n",
       "      <th>SUB_ukpolitics</th>\n",
       "      <th>SUB_unitedkingdom</th>\n",
       "      <th>SUB_videos</th>\n",
       "      <th>SUB_worldnews</th>\n",
       "      <th>SUB_wow</th>\n",
       "      <th>SUB_xboxone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11-01</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I don't pay attention to her, but as long as s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016-09-02 10:35:08</td>\n",
       "      <td>do you find ariana grande sexy ?</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  score  ups  \\\n",
       "0      0                                         NC and NH.      2   -1   \n",
       "1      0  You do know west teams play against west teams...     -4   -1   \n",
       "2      0  They were underdogs earlier today, but since G...      3    3   \n",
       "3      0  This meme isn't funny none of the \"new york ni...     -8   -1   \n",
       "4      0  I don't pay attention to her, but as long as s...      0    0   \n",
       "\n",
       "   downs        date          created_utc  \\\n",
       "0     -1  2016-10-01  2016-10-16 23:55:23   \n",
       "1     -1  2016-11-01  2016-11-01 00:24:10   \n",
       "2      0  2016-09-01  2016-09-22 21:45:37   \n",
       "3     -1  2016-10-01  2016-10-18 21:03:47   \n",
       "4      0  2016-09-01  2016-09-02 10:35:08   \n",
       "\n",
       "                                      parent_comment  year  SUB_2007scape  \\\n",
       "0  Yeah, I get that argument. At this point, I'd ...  2016          False   \n",
       "1  The blazers and Mavericks (The wests 5 and 6 s...  2016          False   \n",
       "2                            They're favored to win.  2016          False   \n",
       "3                         deadass don't kill my buzz  2016          False   \n",
       "4                   do you find ariana grande sexy ?  2016          False   \n",
       "\n",
       "   ...  SUB_television  SUB_tf2  SUB_todayilearned  SUB_trees  SUB_ukpolitics  \\\n",
       "0  ...           False    False              False      False           False   \n",
       "1  ...           False    False              False      False           False   \n",
       "2  ...           False    False              False      False           False   \n",
       "3  ...           False    False              False      False           False   \n",
       "4  ...           False    False              False      False           False   \n",
       "\n",
       "   SUB_unitedkingdom  SUB_videos  SUB_worldnews  SUB_wow  SUB_xboxone  \n",
       "0              False       False          False    False        False  \n",
       "1              False       False          False    False        False  \n",
       "2              False       False          False    False        False  \n",
       "3              False       False          False    False        False  \n",
       "4              False       False          False    False        False  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ee2686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns='parent_comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a34297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94c1c387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lokikeeler/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "ENGLISH_STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "def my_tokenizer(sentence):\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    # remove punctuation and set to lower case\n",
    "    for punctuation_mark in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation_mark,'').lower()\n",
    "        #took out lower case converter\n",
    "\n",
    "    # split sentence into words\n",
    "    listofwords = sentence.split(' ')\n",
    "    listofstemmed_words = []\n",
    "    \n",
    "    # remove stopwords and any tokens that are just empty strings\n",
    "    for word in listofwords:\n",
    "        if (not word in ENGLISH_STOP_WORDS) and (word!=''):\n",
    "            # Stem words\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            listofstemmed_words.append(stemmed_word)\n",
    "\n",
    "    return listofstemmed_words  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50cf62ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lokikeeler/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f4627f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='label')\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11776d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fdc8766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384932, 107)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4773fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb66a2c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48f6ba12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lokikeeler/anaconda3/envs/capstone/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(384932, 63771)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using our custom tokenizer in TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "                        # words must appear at least 5 times\n",
    "                        min_df=5,\n",
    "                        # Stemming, lemmatization, stop word removal, tokenization\n",
    "                        tokenizer=my_tokenizer, \n",
    "                        #ngrams included\n",
    "                        ngram_range=(1, 3)\n",
    "                        )\n",
    "tfidf.fit(X_train['comment'])\n",
    "\n",
    "X_train_transformed = tfidf.transform(X_train['comment'])\n",
    "X_test_transformed = tfidf.transform(X_test['comment'])\n",
    "\n",
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a37715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "096b6afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.6988299232072158\n",
      "Test score: 0.6823703416337318\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "# Fitting a model\n",
    "logreg = LogisticRegression(C=0.1)\n",
    "logreg.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Training and test score\n",
    "print(f\"Train score: {logreg.score(X_train_transformed, y_train)}\")\n",
    "print(f\"Test score: {logreg.score(X_test_transformed, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89ea850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "860577ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarcasm_detector(sentence):   \n",
    "    vector1 = TfidfVectorizer().transform(sentence)\n",
    "    prediction = logreg.predict(vector1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e9a427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_2 = TfidfVectorizer(\n",
    "                        tokenizer=my_tokenizer, \n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff5d0af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarcasm_detector_2(sentence):   \n",
    "    vector1 = tfidf_2.transform(sentence)\n",
    "    prediction = logreg.predict(vector1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70041238",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "The TF-IDF vectorizer is not fitted",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msarcasm_detector\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWow, great lesson today, bud\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m, in \u001b[0;36msarcasm_detector\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msarcasm_detector\u001b[39m(sentence):   \n\u001b[0;32m----> 2\u001b[0m     vector1 \u001b[38;5;241m=\u001b[39m \u001b[43mTfidfVectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m logreg\u001b[38;5;241m.\u001b[39mpredict(vector1)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prediction\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1872\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_documents):\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[1;32m   1858\u001b[0m \n\u001b[1;32m   1859\u001b[0m \u001b[38;5;124;03m    Uses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;124;03m        Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[1;32m   1871\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1872\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mThe TF-IDF vectorizer is not fitted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1874\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtransform(raw_documents)\n\u001b[1;32m   1875\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mtransform(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/sklearn/utils/validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     66\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[1;32m     67\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args],\n\u001b[1;32m     68\u001b[0m                                  args[\u001b[38;5;241m-\u001b[39mextra_args:])]\n",
      "File \u001b[0;32m~/anaconda3/envs/capstone/lib/python3.8/site-packages/sklearn/utils/validation.py:1041\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     attrs \u001b[38;5;241m=\u001b[39m [v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(estimator)\n\u001b[1;32m   1038\u001b[0m              \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m attrs:\n\u001b[0;32m-> 1041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: The TF-IDF vectorizer is not fitted"
     ]
    }
   ],
   "source": [
    "sarcasm_detector(\"Wow, great lesson today, bud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf4367e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f25d441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT training set accuracy: 0.9234436212110191\n",
      "DT test set accuracy: 0.6351320224038018\n"
     ]
    }
   ],
   "source": [
    "# Instantiate & fit the DT\n",
    "DT_model = DecisionTreeClassifier(max_depth=10)\n",
    "DT_model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Evaluate its classification accuracy (Just on the training set for now)\n",
    "print(f\"DT training set accuracy: {DT_model.score(X_train_transformed, y_train)}\")\n",
    "print(f\"DT test set accuracy: {DT_model.score(X_test_transformed, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dff184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44cb2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(\n",
    "    max_depth=20,\n",
    "    min_samples_split=0.05,  # 5% of total data size\n",
    "    min_samples_leaf=0.02,   # 2% of total data size\n",
    "    max_leaf_nodes=100,\n",
    "    max_features='sqrt',\n",
    "    min_impurity_decrease=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fded7e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca4b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decission tree\n",
    "# logisitic\n",
    "# IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726c4b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# took out lower case remover (so it allows it)\n",
    "# TF IDF\n",
    "# CONFUSION MATRICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36273bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEXT SPRINT\n",
    "# improve accuracy of decision tree\n",
    "# improve with random forrest\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
