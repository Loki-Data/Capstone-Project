{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fadaaedd",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e626514",
   "metadata": {},
   "source": [
    "Now that we have completed all of our models, we can finally implement our own sarcasm detector! We will perform this on our best performing model (outside of the BERT), which was a Logistic Regression with a TFIDF vectorization and a tweaked tokenizer. Let's jump right into it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541ddeca",
   "metadata": {},
   "source": [
    "As always, the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a42e3f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647250d0",
   "metadata": {},
   "source": [
    "And the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7d4e9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/lokikeeler/Downloads/train-balanced-sarcasm_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffc96770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>year</th>\n",
       "      <th>SUB_2007scape</th>\n",
       "      <th>...</th>\n",
       "      <th>SUB_television</th>\n",
       "      <th>SUB_tf2</th>\n",
       "      <th>SUB_todayilearned</th>\n",
       "      <th>SUB_trees</th>\n",
       "      <th>SUB_ukpolitics</th>\n",
       "      <th>SUB_unitedkingdom</th>\n",
       "      <th>SUB_videos</th>\n",
       "      <th>SUB_worldnews</th>\n",
       "      <th>SUB_wow</th>\n",
       "      <th>SUB_xboxone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11-01</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I don't pay attention to her, but as long as s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>2016-09-02 10:35:08</td>\n",
       "      <td>do you find ariana grande sexy ?</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  score  ups  \\\n",
       "0      0                                         NC and NH.      2   -1   \n",
       "1      0  You do know west teams play against west teams...     -4   -1   \n",
       "2      0  They were underdogs earlier today, but since G...      3    3   \n",
       "3      0  This meme isn't funny none of the \"new york ni...     -8   -1   \n",
       "4      0  I don't pay attention to her, but as long as s...      0    0   \n",
       "\n",
       "   downs        date          created_utc  \\\n",
       "0     -1  2016-10-01  2016-10-16 23:55:23   \n",
       "1     -1  2016-11-01  2016-11-01 00:24:10   \n",
       "2      0  2016-09-01  2016-09-22 21:45:37   \n",
       "3     -1  2016-10-01  2016-10-18 21:03:47   \n",
       "4      0  2016-09-01  2016-09-02 10:35:08   \n",
       "\n",
       "                                      parent_comment  year  SUB_2007scape  \\\n",
       "0  Yeah, I get that argument. At this point, I'd ...  2016          False   \n",
       "1  The blazers and Mavericks (The wests 5 and 6 s...  2016          False   \n",
       "2                            They're favored to win.  2016          False   \n",
       "3                         deadass don't kill my buzz  2016          False   \n",
       "4                   do you find ariana grande sexy ?  2016          False   \n",
       "\n",
       "   ...  SUB_television  SUB_tf2  SUB_todayilearned  SUB_trees  SUB_ukpolitics  \\\n",
       "0  ...           False    False              False      False           False   \n",
       "1  ...           False    False              False      False           False   \n",
       "2  ...           False    False              False      False           False   \n",
       "3  ...           False    False              False      False           False   \n",
       "4  ...           False    False              False      False           False   \n",
       "\n",
       "   SUB_unitedkingdom  SUB_videos  SUB_worldnews  SUB_wow  SUB_xboxone  \n",
       "0              False       False          False    False        False  \n",
       "1              False       False          False    False        False  \n",
       "2              False       False          False    False        False  \n",
       "3              False       False          False    False        False  \n",
       "4              False       False          False    False        False  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ee2686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns='parent_comment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c35a7a",
   "metadata": {},
   "source": [
    "Here is our tokenizer, the same as the one in our Logistic Regression model with the TFIDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94c1c387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lokikeeler/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "ENGLISH_STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "def my_tokenizer(sentence):\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    for punctuation_mark in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation_mark,'')\n",
    "\n",
    "    listofwords = sentence.split(' ')\n",
    "    listofstemmed_words = []\n",
    "\n",
    "    for word in listofwords:\n",
    "        if (not word in ENGLISH_STOP_WORDS) and (word!=''):\n",
    "            # Stem words\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            listofstemmed_words.append(stemmed_word)\n",
    "\n",
    "    return listofstemmed_words  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b65ea6",
   "metadata": {},
   "source": [
    "Again we will work with the comment column, and create our train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f4627f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='label')\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11776d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fdc8766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384932, 107)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8410243c",
   "metadata": {},
   "source": [
    "Now I will apply the same TFIDF vectorizer with the same parameters as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48f6ba12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lokikeeler/anaconda3/envs/capstone/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(384932, 500)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using our custom tokenizer in TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "                       \n",
    "                        min_df=5,\n",
    "                        max_features=500,\n",
    "                        tokenizer=my_tokenizer, \n",
    "                        ngram_range=(1, 3)\n",
    "                        )\n",
    "tfidf.fit(X_train['comment'])\n",
    "\n",
    "X_train_transformed = tfidf.transform(X_train['comment'])\n",
    "X_test_transformed = tfidf.transform(X_test['comment'])\n",
    "\n",
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df48694",
   "metadata": {},
   "source": [
    "Lastly, fitting the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d89ea850",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35ccbe86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d346420",
   "metadata": {},
   "source": [
    "Perfect. Now we are ready for our sarcasm detector! We will create this function which includes the sentence, the vectorizer (tfidf), and the model (logistic regression). Then our fitted model will make a prediction about whether the setence is sarcastic or not and return a 1 (sarcastic) or 0 (not sarcastic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff5d0af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarcasm_detector(sentence, vectorizer, model):  \n",
    "    vector1 = vectorizer.transform([sentence])\n",
    "    prediction = model.predict(vector1)\n",
    "    return prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46ae0bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarcasm_detector(\"Wow, great lesson today, bud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2569272d",
   "metadata": {},
   "source": [
    "For this first sentence, I meant this to be sarcastic. In reality this sentence could be serious or sarcastic, but I'm glad it got it correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36273bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarcasm_detector(\"That shirt is blue\", tfidf, logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8343a07",
   "metadata": {},
   "source": [
    "Alright, that one was easy, but an important test. Let's give it another sarcastic sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a95009d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarcasm_detector(\"Great haircut, Paul!\", tfidf, logreg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a41d476",
   "metadata": {},
   "source": [
    "And again it gets it correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a8bf2",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9f6039",
   "metadata": {},
   "source": [
    "Using the Logistic Regression with a TFIDF vectorizer from our best model in a previous journal, I was able to create a real sarcasm detector function. I inputted sarcastic and non sarcastic sentences and the model guessed them all correctly! I would have to work hard to try and trick this model.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
